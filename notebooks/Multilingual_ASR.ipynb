{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adwewecat/F8/blob/master/notebooks/Multilingual_ASR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5hvo8QWN-a9"
      },
      "source": [
        "# Installing Whisper\n",
        "\n",
        "The commands below will install the Python packages needed to use Whisper models and evaluate the transcription results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZsJUxc0aRsAf",
        "outputId": "5b575e37-c9a6-4237-bd8b-d99785f47267",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-sjgymmt1\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-sjgymmt1\n",
            "  Resolved https://github.com/openai/whisper.git to commit dd985ac4b90cafeef8712f2998d62c59c3e62d22\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (10.7.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (2.0.2)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (0.9.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (4.67.1)\n",
            "Requirement already satisfied: triton>=2 in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (3.2.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->openai-whisper==20240930) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper==20240930) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper==20240930) (2.32.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->openai-whisper==20240930) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2025.6.15)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->openai-whisper==20240930) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m126.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m102.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m60.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m105.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20240930-py3-none-any.whl size=803707 sha256=bfa4f50aa11d74748fcdd6b6480f80dd4084377bf6dc3e11c533f12715c0b060\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-odgf8e6s/wheels/1f/1d/98/9583695e6695a6ac0ad42d87511097dce5ba486647dbfecb0e\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, openai-whisper\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n"
          ]
        }
      ],
      "source": [
        "! pip install git+https://github.com/openai/whisper.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3CqtR2Fi5-vP"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "try:\n",
        "    import tensorflow  # required in Colab to avoid protobuf compatibility issues\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "import urllib\n",
        "import tarfile\n",
        "import whisper\n",
        "import torchaudio\n",
        "\n",
        "from scipy.io import wavfile\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "\n",
        "pd.options.display.max_rows = 100\n",
        "pd.options.display.max_colwidth = 1000\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IMEkgyagYto"
      },
      "source": [
        "# Loading the Fleurs dataset\n",
        "\n",
        "Select the language of the Fleur dataset to download. Please note that the transcription and translation performance varies widely depending on the language. Appendix D.2 in the paper contains the performance breakdown by language."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L4lPK5106Of2"
      },
      "outputs": [],
      "source": [
        "import ipywidgets as widgets\n",
        "\n",
        "languages = {\"af_za\": \"Afrikaans\", \"am_et\": \"Amharic\", \"ar_eg\": \"Arabic\", \"as_in\": \"Assamese\", \"az_az\": \"Azerbaijani\", \"be_by\": \"Belarusian\", \"bg_bg\": \"Bulgarian\", \"bn_in\": \"Bengali\", \"bs_ba\": \"Bosnian\", \"ca_es\": \"Catalan\", \"cmn_hans_cn\": \"Chinese\", \"cs_cz\": \"Czech\", \"cy_gb\": \"Welsh\", \"da_dk\": \"Danish\", \"de_de\": \"German\", \"el_gr\": \"Greek\", \"en_us\": \"English\", \"es_419\": \"Spanish\", \"et_ee\": \"Estonian\", \"fa_ir\": \"Persian\", \"fi_fi\": \"Finnish\", \"fil_ph\": \"Tagalog\", \"fr_fr\": \"French\", \"gl_es\": \"Galician\", \"gu_in\": \"Gujarati\", \"ha_ng\": \"Hausa\", \"he_il\": \"Hebrew\", \"hi_in\": \"Hindi\", \"hr_hr\": \"Croatian\", \"hu_hu\": \"Hungarian\", \"hy_am\": \"Armenian\", \"id_id\": \"Indonesian\", \"is_is\": \"Icelandic\", \"it_it\": \"Italian\", \"ja_jp\": \"Japanese\", \"jv_id\": \"Javanese\", \"ka_ge\": \"Georgian\", \"kk_kz\": \"Kazakh\", \"km_kh\": \"Khmer\", \"kn_in\": \"Kannada\", \"ko_kr\": \"Korean\", \"lb_lu\": \"Luxembourgish\", \"ln_cd\": \"Lingala\", \"lo_la\": \"Lao\", \"lt_lt\": \"Lithuanian\", \"lv_lv\": \"Latvian\", \"mi_nz\": \"Maori\", \"mk_mk\": \"Macedonian\", \"ml_in\": \"Malayalam\", \"mn_mn\": \"Mongolian\", \"mr_in\": \"Marathi\", \"ms_my\": \"Malay\", \"mt_mt\": \"Maltese\", \"my_mm\": \"Myanmar\", \"nb_no\": \"Norwegian\", \"ne_np\": \"Nepali\", \"nl_nl\": \"Dutch\", \"oc_fr\": \"Occitan\", \"pa_in\": \"Punjabi\", \"pl_pl\": \"Polish\", \"ps_af\": \"Pashto\", \"pt_br\": \"Portuguese\", \"ro_ro\": \"Romanian\", \"ru_ru\": \"Russian\", \"sd_in\": \"Sindhi\", \"sk_sk\": \"Slovak\", \"sl_si\": \"Slovenian\", \"sn_zw\": \"Shona\", \"so_so\": \"Somali\", \"sr_rs\": \"Serbian\", \"sv_se\": \"Swedish\", \"sw_ke\": \"Swahili\", \"ta_in\": \"Tamil\", \"te_in\": \"Telugu\", \"tg_tj\": \"Tajik\", \"th_th\": \"Thai\", \"tr_tr\": \"Turkish\", \"uk_ua\": \"Ukrainian\", \"ur_pk\": \"Urdu\", \"uz_uz\": \"Uzbek\", \"vi_vn\": \"Vietnamese\", \"yo_ng\": \"Yoruba\"}\n",
        "selection = widgets.Dropdown(\n",
        "    options=[(\"Select language\", None), (\"----------\", None)] + sorted([(f\"{v} ({k})\", k) for k, v in languages.items()]),\n",
        "    value=\"ko_kr\",\n",
        "    description='Language:',\n",
        "    disabled=False,\n",
        ")\n",
        "\n",
        "selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4eihI6oK6Of2"
      },
      "outputs": [],
      "source": [
        "lang = selection.value\n",
        "language = languages[lang]\n",
        "\n",
        "assert lang is not None, \"Please select a language\"\n",
        "print(f\"Selected language: {language} ({lang})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GuCCB2KYOJCE"
      },
      "outputs": [],
      "source": [
        "def download(url: str, target_path: str):\n",
        "    with urllib.request.urlopen(url) as source, open(target_path, \"wb\") as output:\n",
        "        with tqdm(total=int(source.info().get(\"Content-Length\")), ncols=80, unit='iB', unit_scale=True, unit_divisor=1024) as loop:\n",
        "            while True:\n",
        "                buffer = source.read(8192)\n",
        "                if not buffer:\n",
        "                    break\n",
        "\n",
        "                output.write(buffer)\n",
        "                loop.update(len(buffer))\n",
        "\n",
        "\n",
        "class Fleurs(torch.utils.data.Dataset):\n",
        "    \"\"\"\n",
        "    A simple class to wrap Fleurs and subsample a portion of the dataset as needed.\n",
        "    \"\"\"\n",
        "    def __init__(self, lang, split=\"test\", subsample_rate=1, device=DEVICE):\n",
        "        url = f\"https://storage.googleapis.com/xtreme_translations/FLEURS102/{lang}.tar.gz\"\n",
        "        tar_path = os.path.expanduser(f\"~/.cache/fleurs/{lang}.tgz\")\n",
        "        os.makedirs(os.path.dirname(tar_path), exist_ok=True)\n",
        "\n",
        "        if not os.path.exists(tar_path):\n",
        "            download(url, tar_path)\n",
        "\n",
        "        all_audio = {}\n",
        "        with tarfile.open(tar_path, \"r:gz\") as tar:\n",
        "            for member in tar.getmembers():\n",
        "                name = member.name\n",
        "                if name.endswith(f\"{split}.tsv\"):\n",
        "                    labels = pd.read_table(tar.extractfile(member), names=(\"id\", \"file_name\", \"raw_transcription\", \"transcription\", \"_\", \"num_samples\", \"gender\"))\n",
        "\n",
        "                if f\"/{split}/\" in name and name.endswith(\".wav\"):\n",
        "                    audio_bytes = tar.extractfile(member).read()\n",
        "                    all_audio[os.path.basename(name)] = wavfile.read(io.BytesIO(audio_bytes))[1]\n",
        "\n",
        "        self.labels = labels.to_dict(\"records\")[::subsample_rate]\n",
        "        self.all_audio = all_audio\n",
        "        self.device = device\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        record = self.labels[item]\n",
        "        audio = torch.from_numpy(self.all_audio[record[\"file_name\"]].copy())\n",
        "        text = record[\"transcription\"]\n",
        "\n",
        "        return (audio, text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-YcRU5jqNqo2"
      },
      "outputs": [],
      "source": [
        "dataset = Fleurs(lang, subsample_rate=10)  # subsample 10% of the dataset for a quick demo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ljocCNuUAde"
      },
      "source": [
        "# Running inference on the dataset using a medium Whisper model\n",
        "\n",
        "The following will take a few minutes to transcribe and translate utterances in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_PokfNJtOYNu"
      },
      "outputs": [],
      "source": [
        "model = whisper.load_model(\"medium\")\n",
        "print(\n",
        "    f\"Model is {'multilingual' if model.is_multilingual else 'English-only'} \"\n",
        "    f\"and has {sum(np.prod(p.shape) for p in model.parameters()):,} parameters.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F74Yfr696Of5"
      },
      "outputs": [],
      "source": [
        "options = dict(language=language, beam_size=5, best_of=5)\n",
        "transcribe_options = dict(task=\"transcribe\", **options)\n",
        "translate_options = dict(task=\"translate\", **options)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7OWTn_KvNk59",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "references = []\n",
        "transcriptions = []\n",
        "translations = []\n",
        "\n",
        "for audio, text in tqdm(dataset):\n",
        "    transcription = model.transcribe(audio, **transcribe_options)[\"text\"]\n",
        "    translation = model.transcribe(audio, **translate_options)[\"text\"]\n",
        "\n",
        "    transcriptions.append(transcription)\n",
        "    translations.append(translation)\n",
        "    references.append(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4nTyynELQ42j",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "data = pd.DataFrame(dict(reference=references, transcription=transcriptions, translation=translations))\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83LRfd1plYEb"
      },
      "source": [
        "# Word-level timestamps using attention weights\n",
        "\n",
        "Below, we use the cross-attention weights to determine more granular, word-level timestamps. It uses a set of heuristics and dynamic time warping (DTW) to find the alignment between the audio and the transcript."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KGtZWNaQlOVC"
      },
      "outputs": [],
      "source": [
        "! pip install dtw-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3HTv8KmzlZtc"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.font_manager as fm\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "from IPython.display import display, HTML\n",
        "from whisper.tokenizer import get_tokenizer\n",
        "from dtw import dtw\n",
        "from scipy.ndimage import median_filter\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = \"retina\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jqSiARgqlb6X"
      },
      "outputs": [],
      "source": [
        "AUDIO_SAMPLES_PER_TOKEN = whisper.audio.HOP_LENGTH * 2\n",
        "AUDIO_TIME_PER_TOKEN = AUDIO_SAMPLES_PER_TOKEN / whisper.audio.SAMPLE_RATE\n",
        "\n",
        "medfilt_width = 7\n",
        "qk_scale = 1.0\n",
        "\n",
        "tokenizer = get_tokenizer(model.is_multilingual, language=languages[lang])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZTVen0YkldgU"
      },
      "outputs": [],
      "source": [
        "# This part downloads a repackaged version of the Noto Sans font (either CJK or non-CJK)\n",
        "# to render various languages in Matplotlib figures.\n",
        "\n",
        "if languages[lang] in {\"Chinese\", \"Japanese\", \"Korean\"}:\n",
        "    font = \"GoNotoCJKCore.ttf\"\n",
        "else:\n",
        "    font = \"GoNotoCurrent.ttf\"\n",
        "\n",
        "font_release = \"https://github.com/satbyy/go-noto-universal/releases/download/v5.2\"\n",
        "if not os.path.exists(font):\n",
        "    download(f\"{font_release}/{font}\", font)\n",
        "\n",
        "prop = fm.FontProperties(fname=font)\n",
        "props = {'fontproperties': prop}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UiOFv8X5lhQA"
      },
      "outputs": [],
      "source": [
        "def split_tokens_on_unicode(tokens: torch.Tensor):\n",
        "    words = []\n",
        "    word_tokens = []\n",
        "    current_tokens = []\n",
        "\n",
        "    for token in tokens.tolist():\n",
        "        current_tokens.append(token)\n",
        "        decoded = tokenizer.decode_with_timestamps(current_tokens)\n",
        "        if \"\\ufffd\" not in decoded:\n",
        "            words.append(decoded)\n",
        "            word_tokens.append(current_tokens)\n",
        "            current_tokens = []\n",
        "\n",
        "    return words, word_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CkhsL9xUmHjt"
      },
      "outputs": [],
      "source": [
        "def split_tokens_on_spaces(tokens: torch.Tensor):\n",
        "    subwords, subword_tokens_list = split_tokens_on_unicode(tokens)\n",
        "    words = []\n",
        "    word_tokens = []\n",
        "\n",
        "    for subword, subword_tokens in zip(subwords, subword_tokens_list):\n",
        "        special = subword_tokens[0] >= tokenizer.eot\n",
        "        with_space = subword.startswith(\" \")\n",
        "        punctuation = subword.strip() in string.punctuation\n",
        "        if special or with_space or punctuation:\n",
        "            words.append(subword)\n",
        "            word_tokens.append(subword_tokens)\n",
        "        else:\n",
        "            words[-1] = words[-1] + subword\n",
        "            word_tokens[-1].extend(subword_tokens)\n",
        "\n",
        "    return words, word_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQOl_4TSmIgB"
      },
      "outputs": [],
      "source": [
        "if languages[lang] in {\"Chinese\", \"Japanese\", \"Thai\", \"Lao\", \"Myanmar\"}:\n",
        "    # These languages don't typically use spaces, so it is difficult to split words\n",
        "    # without morpheme analysis. Here, we instead split words at any\n",
        "    # position where the tokens are decoded as valid unicode points\n",
        "    split_tokens = split_tokens_on_unicode\n",
        "else:\n",
        "    split_tokens = split_tokens_on_spaces"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install hooks on the cross attention layers to retrieve the attention weights\n",
        "QKs = [None] * model.dims.n_text_layer\n",
        "\n",
        "for i, block in enumerate(model.decoder.blocks):\n",
        "    block.cross_attn.register_forward_hook(\n",
        "        lambda _, ins, outs, index=i: QKs.__setitem__(index, outs[-1])\n",
        "    )"
      ],
      "metadata": {
        "id": "DGwrymCo5SUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6q-mBLVNmJ1i"
      },
      "outputs": [],
      "source": [
        "# for the first 10 examples in the dataset\n",
        "for (audio, label), transcription in zip(dataset, transcriptions[:10]):\n",
        "    print(transcription)\n",
        "\n",
        "    duration = len(audio)\n",
        "    mel = whisper.log_mel_spectrogram(whisper.pad_or_trim(audio)).cuda()\n",
        "    tokens = torch.tensor(\n",
        "        [\n",
        "            *tokenizer.sot_sequence,\n",
        "            tokenizer.timestamp_begin,\n",
        "        ] + tokenizer.encode(transcription) + [\n",
        "            tokenizer.timestamp_begin + duration // AUDIO_SAMPLES_PER_TOKEN,\n",
        "            tokenizer.eot,\n",
        "        ]\n",
        "    ).cuda()\n",
        "    with torch.no_grad():\n",
        "        logits = model(mel.unsqueeze(0), tokens.unsqueeze(0))\n",
        "\n",
        "    weights = torch.cat(QKs)  # layers * heads * tokens * frames\n",
        "    weights = weights[:, :, :, : duration // AUDIO_SAMPLES_PER_TOKEN].cpu()\n",
        "    weights = median_filter(weights, (1, 1, 1, medfilt_width))\n",
        "    weights = torch.tensor(weights * qk_scale).softmax(dim=-1)\n",
        "\n",
        "    w = weights / weights.norm(dim=-2, keepdim=True)\n",
        "    matrix = w[-6:].mean(axis=(0, 1))\n",
        "\n",
        "    alignment = dtw(-matrix.double().numpy())\n",
        "\n",
        "    jumps = np.pad(np.diff(alignment.index1s), (1, 0), constant_values=1).astype(bool)\n",
        "    jump_times = alignment.index2s[jumps] * AUDIO_TIME_PER_TOKEN\n",
        "    words, word_tokens = split_tokens(tokens)\n",
        "\n",
        "    # display the normalized attention weights and the alignment\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.imshow(matrix, aspect=\"auto\")\n",
        "    plt.plot(alignment.index2s, alignment.index1s, color=\"red\")\n",
        "\n",
        "    xticks = np.arange(0, matrix.shape[1], 1 / AUDIO_TIME_PER_TOKEN)\n",
        "    xticklabels = (xticks * AUDIO_TIME_PER_TOKEN).round().astype(np.int32)\n",
        "    plt.xticks(xticks, xticklabels)\n",
        "    plt.xlabel(\"Time (s)\")\n",
        "\n",
        "    # display tokens and words as tick labels\n",
        "    ylims = plt.gca().get_ylim()\n",
        "\n",
        "    ax = plt.gca()\n",
        "    ax.tick_params('both', length=0, width=0, which='minor', pad=6)\n",
        "\n",
        "    ax.yaxis.set_ticks_position(\"left\")\n",
        "    ax.yaxis.set_label_position(\"left\")\n",
        "    ax.invert_yaxis()\n",
        "    ax.set_ylim(ylims)\n",
        "\n",
        "    major_ticks = [-0.5]\n",
        "    minor_ticks = []\n",
        "    current_y = 0\n",
        "\n",
        "    for word, word_token in zip(words, word_tokens):\n",
        "        minor_ticks.append(current_y + len(word_token) / 2 - 0.5)\n",
        "        current_y += len(word_token)\n",
        "        major_ticks.append(current_y - 0.5)\n",
        "\n",
        "    ax.yaxis.set_minor_locator(ticker.FixedLocator(minor_ticks))\n",
        "    ax.yaxis.set_minor_formatter(ticker.FixedFormatter(words))\n",
        "    ax.set_yticks(major_ticks)\n",
        "    ax.yaxis.set_major_formatter(ticker.NullFormatter())\n",
        "\n",
        "    for label in ax.get_yminorticklabels():\n",
        "        label.set_fontproperties(prop)\n",
        "\n",
        "    plt.ylabel(\"Words\")\n",
        "    plt.show()\n",
        "\n",
        "    # display the word-level timestamps in a table\n",
        "    word_boundaries = np.pad(np.cumsum([len(t) for t in word_tokens[:-1]]), (1, 0))\n",
        "    begin_times = jump_times[word_boundaries[:-1]]\n",
        "    end_times = jump_times[word_boundaries[1:]]\n",
        "\n",
        "    data = [\n",
        "        dict(word=word, begin=begin, end=end)\n",
        "        for word, begin, end in zip(words[:-1], begin_times, end_times)\n",
        "        if not word.startswith(\"<|\") and word.strip() not in \".,!?、。\"\n",
        "    ]\n",
        "\n",
        "    display(pd.DataFrame(data))\n",
        "    display(HTML(\"<hr>\"))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}